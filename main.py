import streamlit as st
import pandas as pd
import google.generativeai as genai
import time

# --- CONFIGURACI√ìN DE P√ÅGINA ---
st.set_page_config(page_title="Lucho | Pedro Bravin", page_icon="üèóÔ∏è", layout="centered")

# --- 1. CONEXI√ìN SEGURA CON GOOGLE ---
try:
    # Busca la clave en los secretos de Streamlit
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=API_KEY)
except Exception:
    st.error("üö® Error Cr√≠tico: No se encontr√≥ la API KEY en los Secrets de Streamlit.")
    st.stop()

# --- 2. CONFIGURACI√ìN DEL MODELO ---
# Usamos el modelo m√°s compatible y r√°pido
MODELO_ELEGIDO = "gemini-1.5-flash"

generation_config = {
  "temperature": 0.7,
  "top_p": 0.95,
  "top_k": 64,
  "max_output_tokens": 8192,
}

# --- 3. BASE DE DATOS (PRECIOS) ---
SHEET_URL = "https://docs.google.com/spreadsheets/d/e/2PACX-1vTgHzHMiNP9jH7vBAkpYiIVCzUaFbNKLC8_R9ZpwIbgMc7suQMR7yActsCdkww1VxtgBHcXOv4EGvXj/pub?gid=1937732333&single=true&output=csv"

@st.cache_data(ttl=600)
def cargar_precios():
    try:
        df = pd.read_csv(SHEET_URL, encoding='utf-8', on_bad_lines='skip')
        return df.to_string(index=False)
    except:
        return "Error al cargar la lista de precios."

csv_context = cargar_precios()

# --- 4. EL CEREBRO DE LUCHO (PROMPT V72) ---
SYSTEM_PROMPT = f"""
ROL: Eres Lucho, Ejecutivo Comercial Senior de Pedro Bravin Materiales.
OBJETIVO: Cotizar r√°pido, maximizar ticket y derivar a WhatsApp.

BASE DE DATOS (TU MEMORIA):
{csv_context}

REGLAS DE INTERACCI√ìN:
1. Saludo: "Hola, buenas [ma√±anas/tardes]."
2. PROACTIVIDAD: "¬øQu√© proyecto ten√©s? ¬øTechado, rejas, pintura o construcci√≥n?"
3. CANDADO DE DATOS: Antes de dar precio final, pregunt√°: "¬øTu Nombre y Localidad? (Para chequear env√≠o gratis)".
4. L√çMITE: T√∫ solo reservas pedidos.

MATEM√ÅTICA Y PRODUCTOS:
* IVA: Precios del CSV son NETOS. **MULTIPLICA SIEMPRE POR 1.21**.
* TUBOS: Se venden por tira de 6.40m (Conducci√≥n) o 6.00m (Estructural).
* PLANCHUELAS: Precio por Unidad.
* AISLANTES: <$10k es x m2. >$10k es x rollo.

PROTOCOLOS DE VENTA:
* CHAPAS: Filtro Techo vs Lisa. Aislante Consultiva. Acopio Bolsa de Metros.
* TEJIDOS: Kit Completo (Eco -> Acindar).
* CONSTRUCCI√ìN: Hierro ADN vs Liso. Upsell Alambre/Clavos.
* NO LISTADO: "Consulto stock en dep√≥sito".

CIERRE Y WHATSAPP:
1. Pedir: Nombre, CUIT, Tel√©fono.
2. Link WhatsApp con resumen.
   [‚úÖ ENVIAR PEDIDO CONFIRMADO](LINK)
   "üìç Retiro: [LINK_MAPS]"
"""

# --- 5. INTERFAZ DE CHAT ---
st.title("üèóÔ∏è Habl√° con Lucho")
st.markdown("**Atenci√≥n Comercial | Pedro Bravin**")

if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "model", "content": "Hola, buenas. Soy Lucho. ¬øQu√© proyecto ten√©s hoy? ¬øTechado, rejas, pintura o construcci√≥n?"}
    ]

# Mostrar historial
for message in st.session_state.messages:
    avatar = "üë∑‚Äç‚ôÇÔ∏è" if message["role"] == "model" else "üë§"
    with st.chat_message(message["role"], avatar=avatar):
        st.markdown(message["content"])

# Capturar input y responder
if prompt := st.chat_input("Escrib√≠ ac√°..."):
    # Mostrar usuario
    with st.chat_message("user", avatar="üë§"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # Generar respuesta
    try:
        # Configurar modelo con el prompt de sistema
        model = genai.GenerativeModel(
            model_name=MODELO_ELEGIDO,
            system_instruction=SYSTEM_PROMPT
        )
        
        # Convertir historial al formato de la librer√≠a cl√°sica
        chat_history = []
        for m in st.session_state.messages:
            if m["role"] != "system": # Ignoramos el system prompt en el historial
                role = "user" if m["role"] == "user" else "model"
                chat_history.append({"role": role, "parts": [m["content"]]})

        # Iniciar chat
        chat = model.start_chat(history=chat_history)
        response = chat.send_message(prompt)
        
        # Mostrar respuesta
        text_response = response.text
        with st.chat_message("model", avatar="üë∑‚Äç‚ôÇÔ∏è"):
            st.markdown(text_response)
        st.session_state.messages.append({"role": "model", "content": text_response})

    except Exception as e:
        error_msg = str(e)
        if "429" in error_msg:
            st.warning("‚è≥ Estamos recibiendo muchas consultas. Por favor esper√° 10 segundos e intent√° de nuevo.")
        else:
            st.error(f"‚ùå Error de conexi√≥n: {error_msg}")
